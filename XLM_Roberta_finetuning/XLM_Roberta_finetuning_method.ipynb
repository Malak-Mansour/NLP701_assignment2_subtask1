{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:58:04.959761Z",
     "iopub.status.busy": "2024-12-07T05:58:04.959358Z",
     "iopub.status.idle": "2024-12-07T05:58:13.871093Z",
     "shell.execute_reply": "2024-12-07T05:58:13.870348Z",
     "shell.execute_reply.started": "2024-12-07T05:58:04.959725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModel,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-07T05:58:39.214989Z",
     "iopub.status.busy": "2024-12-07T05:58:39.214044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3004961547400bad37de8a3db21fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065c255f86bb4e668f57e06dd5477eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdf51cb30e04dcd84f724f81bd70547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e940d39d81e543c484d9c7b117cafca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e7ca5e6852410a8f62551ae2a0be70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_246/3075952029.py:201: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['Adversary', 'Foreign'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='317' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 317/3290 05:09 < 48:38, 1.02 it/s, Epoch 0.96/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.801500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.777900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.770100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.749900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.752100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.761500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.755200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.740600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/training_data_16_October_release/subtask-1-annotations.txt\"\n",
    "train_raw_documents_folder = \"/kaggle/input/processed-5/assignment2_processed_dataset/training_data_16_October_release/raw-documents\"\n",
    "\n",
    "# Utility Functions\n",
    "def extract_entity_and_fine_grained_roles(parts):\n",
    "    article_id = parts[0]\n",
    "    entity = parts[1]\n",
    "    i = 2\n",
    "    while not parts[i].isdigit():\n",
    "        entity += \" \" + parts[i]\n",
    "        i += 1\n",
    "    remaining_parts = parts[i:]\n",
    "\n",
    "    parts=[]\n",
    "    parts = [article_id, entity, remaining_parts[0], remaining_parts[1]]\n",
    "\n",
    "    if len(remaining_parts) != 2:  # Training set (includes roles)\n",
    "        parts.append(remaining_parts[2])\n",
    "        parts.append(\" \".join(str(item) for item in remaining_parts[3:]))\n",
    "    \n",
    "    return parts\n",
    "\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    processed_lines = [\n",
    "        extract_entity_and_fine_grained_roles(line.strip().split(\"\\t\"))\n",
    "        for line in lines\n",
    "    ]\n",
    "    return pd.DataFrame(\n",
    "        processed_lines,\n",
    "        columns=[\n",
    "            \"article_id\",\n",
    "            \"entity\",\n",
    "            \"start_offset\",\n",
    "            \"end_offset\",\n",
    "            \"main_role\",\n",
    "            \"fine_grained_roles\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def load_document(article_id, folder_path):\n",
    "    file_path = os.path.join(folder_path, article_id)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "# Data Preprocessing: Adding special token around entities\n",
    "def preprocess_entity(entity, context, start_offset, end_offset, tokenizer):\n",
    "    context_with_entity = context[:start_offset] + f\"<ENTITY>{entity}</ENTITY>\" + context[end_offset:]\n",
    "    inputs = tokenizer(\n",
    "        context_with_entity,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Dataset Class with Hierarchical Classification\n",
    "class RoleDataset(Dataset):\n",
    "    def __init__(self, annotations, documents, tokenizer, max_length=512):\n",
    "        self.annotations = annotations\n",
    "        self.documents = documents\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations.iloc[idx]\n",
    "        article_id = annotation[\"article_id\"]\n",
    "        entity = annotation[\"entity\"]\n",
    "        start_offset = int(annotation[\"start_offset\"])\n",
    "        end_offset = int(annotation[\"end_offset\"])\n",
    "        main_role = annotation[\"main_role\"]\n",
    "        fine_grained_roles = annotation[\"fine_grained_roles\"].split(\" \")\n",
    "\n",
    "        # Get the document text\n",
    "        document = self.documents[article_id]\n",
    "\n",
    "        # Preprocess document to add special entity tokens\n",
    "        inputs = preprocess_entity(entity, document, start_offset, end_offset, self.tokenizer)\n",
    "\n",
    "        # Label encoding\n",
    "        all_main_roles = set(main_roles)\n",
    "        all_fine_grained_roles = set(individual_fine_grained_roles)\n",
    "\n",
    "        main_role_label = [main_role]\n",
    "        fine_grained_roles_labels = fine_grained_roles\n",
    "\n",
    "        mlb_main = MultiLabelBinarizer(classes=list(all_main_roles))\n",
    "        mlb_fine = MultiLabelBinarizer(classes=list(all_fine_grained_roles))\n",
    "\n",
    "        main_role_encoded = mlb_main.fit_transform([main_role_label])[0]\n",
    "        fine_grained_encoded = mlb_fine.fit_transform([fine_grained_roles_labels])[0]\n",
    "\n",
    "        # Combine labels\n",
    "        labels = torch.cat(\n",
    "            [\n",
    "                torch.tensor(main_role_encoded, dtype=torch.float),\n",
    "                torch.tensor(fine_grained_encoded, dtype=torch.float),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "# Main Model with Hierarchical Classification\n",
    "class RoleClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_main_roles, num_fine_grained_roles):\n",
    "        super(RoleClassifier, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        self.main_role_classifier = nn.Linear(hidden_size, num_main_roles)\n",
    "        self.fine_grained_classifier = nn.Linear(hidden_size, num_fine_grained_roles)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token representation\n",
    "\n",
    "        main_role_logits = self.main_role_classifier(cls_output)\n",
    "        fine_grained_logits = self.fine_grained_classifier(cls_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            main_role_labels = labels[:, :3]  # First 3 for main roles\n",
    "            fine_grained_labels = labels[:, 3:]  # Remaining for fine-grained roles\n",
    "\n",
    "            main_loss = BCEWithLogitsLoss()(main_role_logits, main_role_labels)\n",
    "            fine_loss = BCEWithLogitsLoss()(fine_grained_logits, fine_grained_labels)\n",
    "\n",
    "            loss = main_loss + fine_loss\n",
    "            return loss, main_role_logits, fine_grained_logits\n",
    "\n",
    "        return main_role_logits, fine_grained_logits\n",
    "\n",
    "\n",
    "# Initialize the tokenizer and dataset\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load annotations and documents\n",
    "annotations = load_annotations(train_annotations_file)\n",
    "documents = {\n",
    "    article_id: load_document(article_id, train_raw_documents_folder)\n",
    "    for article_id in annotations[\"article_id\"].unique()\n",
    "}\n",
    "\n",
    "train_dataset = RoleDataset(annotations, documents, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fine-grained roles and main roles\n",
    "main_roles = [\"Protagonist\", \"Antagonist\", \"Innocent\"]\n",
    "\n",
    "individual_fine_grained_roles = ['Guardian', 'Martyr', 'Peacemaker', 'Rebel', 'Underdog', 'Virtuous',\n",
    "            'Instigator', 'Conspirator', 'Tyrant', 'Foreign Adversary', 'Traitor', \n",
    "            'Spy', 'Saboteur', 'Corrupt', 'Incompetent', 'Terrorist', 'Deceiver', 'Bigot',\n",
    "            'Forgotten', 'Exploited', 'Victim', 'Scapegoat']\n",
    "\n",
    "fine_grained_roles = {\n",
    "    \"Protagonist\": [\"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Underdog\", \"Virtuous\"],\n",
    "    \"Antagonist\": [\"Instigator\", \"Conspirator\", \"Tyrant\", \"Foreign Adversary\", \"Traitor\", \n",
    "                   \"Spy\", \"Saboteur\", \"Corrupt\", \"Incompetent\", \"Terrorist\", \"Deceiver\", \"Bigot\"],\n",
    "    \"Innocent\": [\"Forgotten\", \"Exploited\", \"Victim\", \"Scapegoat\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_main_roles = len(main_roles)\n",
    "num_fine_grained_roles = len(individual_fine_grained_roles)\n",
    "\n",
    "\n",
    "model = RoleClassifier(model_name, num_main_roles, num_fine_grained_roles)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"./role_classifier/pytorch_model.bin\")\n",
    "\n",
    "tokenizer.save_pretrained(\"./role_classifier\")\n",
    "\n",
    "\n",
    "# Save the model configuration\n",
    "model_config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"num_main_roles\": num_main_roles,\n",
    "    \"num_fine_grained_roles\": num_fine_grained_roles,\n",
    "}\n",
    "torch.save(model_config, \"./role_classifier/model_config.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example usage with model loading and prediction\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RoleClassifier(model_name, num_main_roles, num_fine_grained_roles)\n",
    "model.load_state_dict(torch.load(\"./role_classifier/pytorch_model.bin\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dev_raw_documents_folder = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-documents\"\n",
    "\n",
    "sigmoid_threshold = 0.1\n",
    "\n",
    "# Load dev annotations\n",
    "def load_dev_annotations(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    processed_lines = [\n",
    "        extract_entity_and_fine_grained_roles(line.strip().split(\"\\t\"))\n",
    "        for line in lines\n",
    "    ]\n",
    "    return pd.DataFrame(\n",
    "        processed_lines, columns=[\"article_id\", \"entity\", \"start_offset\", \"end_offset\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Dev Dataset\n",
    "class DevRoleDataset(Dataset):\n",
    "    def __init__(self, annotations, documents, tokenizer, max_length=512):\n",
    "        self.annotations = annotations\n",
    "        self.documents = documents\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations.iloc[idx]\n",
    "        article_id = annotation[\"article_id\"]\n",
    "        entity = annotation[\"entity\"]\n",
    "        start_offset = int(annotation[\"start_offset\"])\n",
    "        end_offset = int(annotation[\"end_offset\"])\n",
    "\n",
    "        \n",
    "        # Get the document text\n",
    "        document = self.documents[article_id]\n",
    "\n",
    "\n",
    "        # ENTITY CENTER\n",
    "# Center the entity in the context window\n",
    "        entity_start_char_idx = start_offset\n",
    "        entity_end_char_idx = end_offset\n",
    "\n",
    "        # Define a character window around the entity\n",
    "        half_window_size = self.max_length // 2\n",
    "        left_context_start = max(0, entity_start_char_idx - half_window_size)\n",
    "        right_context_end = min(len(document), entity_end_char_idx + half_window_size)\n",
    "\n",
    "        # Slice the document around the entity\n",
    "        context = document[left_context_start:right_context_end]\n",
    "\n",
    "        # Adjust the entity offsets for the new context\n",
    "        adjusted_entity_start = entity_start_char_idx - left_context_start\n",
    "        adjusted_entity_end = entity_end_char_idx - left_context_start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        context_with_entity = context[:start_offset] + f\"<ENTITY>{entity}</ENTITY>\" + context[end_offset:]\n",
    "\n",
    "        \n",
    "        \n",
    "        # Tokenize and prepare inputs\n",
    "        inputs = self.tokenizer(\n",
    "            # document,\n",
    "            # context,\n",
    "            context_with_entity,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "\n",
    "            \"article_id\": article_id,\n",
    "            \"entity\": entity,\n",
    "            \"start_offset\": start_offset,\n",
    "            \"end_offset\": end_offset,\n",
    "        }\n",
    "\n",
    "\n",
    "def predict_roles_hierarchical(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "            # Main role prediction (first model for coarse classification)\n",
    "            main_role_logits, fine_grained_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Apply softmax for main role classification\n",
    "            main_role_probs = softmax(main_role_logits, dim=-1).cpu().numpy()\n",
    "            main_role_preds = np.argmax(main_role_probs, axis=-1)\n",
    "\n",
    "            # Fine-grained role prediction (based on the predicted main role)\n",
    "            for i in range(len(input_ids)):\n",
    "                main_role = main_roles[main_role_preds[i]]\n",
    "                fine_grained_logits_for_role = fine_grained_logits[i]\n",
    "                fine_grained_preds = (torch.sigmoid(fine_grained_logits_for_role) > sigmoid_threshold).cpu().numpy()\n",
    "\n",
    "                # Map fine-grained roles based on the predicted main role\n",
    " \n",
    "                # Example: A mapping of main roles to their index ranges in fine_grained_preds\n",
    "                # Replace these ranges with the actual ranges for your use case\n",
    "                main_role_index_bounds = {\n",
    "                    \"Protagonist\": (0, 6),  # Indices for Protagonist fine-grained roles\n",
    "                    \"Antagonist\": (6, 12),  # Indices for Antagonist fine-grained roles\n",
    "                    \"Innocent\": (12, 18),   # Indices for Innocent fine-grained roles\n",
    "                    # Add other roles and their bounds here\n",
    "                }\n",
    "                \n",
    "                # Get the start and end index for the main_role\n",
    "                start_idx, end_idx = main_role_index_bounds[main_role]\n",
    "                \n",
    "                # Filter fine-grained roles for the main_role based on its index range\n",
    "                fine_grained_roles_predicted = [\n",
    "                    fine_grained_roles[main_role][idx - start_idx]  # Adjust index relative to main_role's fine-grained roles\n",
    "                    for idx, val in enumerate(fine_grained_preds[start_idx:end_idx])  # Iterate only over the range for the main_role\n",
    "                    if val == 1  # Pick only the roles predicted as True\n",
    "                ]\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "                # Collect results\n",
    "                predictions.append(\n",
    "                    {\n",
    "                        \"article_id\": batch[\"article_id\"][i],\n",
    "                        \"entity\": batch[\"entity\"][i],\n",
    "                        \"start_offset\": batch[\"start_offset\"][i],\n",
    "                        \"end_offset\": batch[\"end_offset\"][i],\n",
    "                        \"main_roles\": [main_roles[main_role_preds[i]]],\n",
    "                        \"fine_grained_roles\": fine_grained_roles_predicted,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "# Update the prediction function to save predictions in the desired format\n",
    "def save_predictions(predictions, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for prediction in predictions:\n",
    "            article_id = prediction[\"article_id\"]\n",
    "            entity = prediction[\"entity\"]\n",
    "            start_offset = prediction[\"start_offset\"]  # Start offset for the entity\n",
    "            end_offset = prediction[\"end_offset\"]  # End offset for the entity\n",
    "            main_role = prediction[\"main_roles\"][0]  # Assume we are picking only one main_role\n",
    "            fine_grained_roles = \"\\t\".join(prediction[\"fine_grained_roles\"])  # Join fine-grained roles with tab\n",
    "\n",
    "            # Prepare the line in the required format\n",
    "            line = f\"{article_id}\\t{entity}\\t{start_offset}\\t{end_offset}\\t{main_role}\\t{fine_grained_roles}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "    print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dev_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-entity-mentions_EN.txt\"\n",
    "\n",
    "\n",
    "# Load dev annotations and documents\n",
    "dev_annotations = load_dev_annotations(dev_annotations_file)\n",
    "dev_documents = {\n",
    "    article_id: load_document(article_id, dev_raw_documents_folder)\n",
    "    for article_id in dev_annotations[\"article_id\"].unique()\n",
    "}\n",
    "\n",
    "# Dev Dataset and DataLoader\n",
    "dev_dataset = DevRoleDataset(dev_annotations, dev_documents, tokenizer)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `dev_loader` is the DataLoader for the dev set\n",
    "predictions = predict_roles_hierarchical(model, dev_loader, tokenizer)\n",
    "\n",
    "# Save predictions to .txt file\n",
    "save_predictions(predictions,\"submission_EN.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dev_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-entity-mentions_BG.txt\"\n",
    "\n",
    "\n",
    "# Load dev annotations and documents\n",
    "dev_annotations = load_dev_annotations(dev_annotations_file)\n",
    "dev_documents = {\n",
    "    article_id: load_document(article_id, dev_raw_documents_folder)\n",
    "    for article_id in dev_annotations[\"article_id\"].unique()\n",
    "}\n",
    "\n",
    "# Dev Dataset and DataLoader\n",
    "dev_dataset = DevRoleDataset(dev_annotations, dev_documents, tokenizer)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `dev_loader` is the DataLoader for the dev set\n",
    "predictions = predict_roles_hierarchical(model, dev_loader, tokenizer)\n",
    "\n",
    "# Save predictions to .txt file\n",
    "save_predictions(predictions,\"submission_BG.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dev_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-entity-mentions_HI.txt\"\n",
    "\n",
    "\n",
    "# Load dev annotations and documents\n",
    "dev_annotations = load_dev_annotations(dev_annotations_file)\n",
    "dev_documents = {\n",
    "    article_id: load_document(article_id, dev_raw_documents_folder)\n",
    "    for article_id in dev_annotations[\"article_id\"].unique()\n",
    "}\n",
    "\n",
    "# Dev Dataset and DataLoader\n",
    "dev_dataset = DevRoleDataset(dev_annotations, dev_documents, tokenizer)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `dev_loader` is the DataLoader for the dev set\n",
    "predictions = predict_roles_hierarchical(model, dev_loader, tokenizer)\n",
    "\n",
    "# Save predictions to .txt file\n",
    "save_predictions(predictions,\"submission_HI.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dev_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-entity-mentions_PT.txt\"\n",
    "\n",
    "\n",
    "# Load dev annotations and documents\n",
    "dev_annotations = load_dev_annotations(dev_annotations_file)\n",
    "dev_documents = {\n",
    "    article_id: load_document(article_id, dev_raw_documents_folder)\n",
    "    for article_id in dev_annotations[\"article_id\"].unique()\n",
    "}\n",
    "\n",
    "# Dev Dataset and DataLoader\n",
    "dev_dataset = DevRoleDataset(dev_annotations, dev_documents, tokenizer)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "# Assuming `dev_loader` is the DataLoader for the dev set\n",
    "predictions = predict_roles_hierarchical(model, dev_loader, tokenizer)\n",
    "\n",
    "# Save predictions to .txt file\n",
    "save_predictions(predictions,\"submission_PT.txt\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6246151,
     "sourceId": 10122381,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6246688,
     "sourceId": 10123104,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6246740,
     "sourceId": 10123191,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6248454,
     "sourceId": 10125706,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
