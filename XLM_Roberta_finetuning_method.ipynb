{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10122381,"sourceType":"datasetVersion","datasetId":6246151},{"sourceId":10123104,"sourceType":"datasetVersion","datasetId":6246688},{"sourceId":10123191,"sourceType":"datasetVersion","datasetId":6246740},{"sourceId":10125706,"sourceType":"datasetVersion","datasetId":6248454}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom transformers import (\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    AutoModel,\n)\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\nfrom torch.nn.functional import softmax\n\n\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/training_data_16_October_release/subtask-1-annotations.txt\"\ntrain_raw_documents_folder = \"/kaggle/input/processed-5/assignment2_processed_dataset/training_data_16_October_release/raw-documents\"\n\n# Utility Functions\ndef extract_entity_and_fine_grained_roles(parts):\n    article_id = parts[0]\n    entity = parts[1]\n    i = 2\n    while not parts[i].isdigit():\n        entity += \" \" + parts[i]\n        i += 1\n    remaining_parts = parts[i:]\n\n    parts=[]\n    parts = [article_id, entity, remaining_parts[0], remaining_parts[1]]\n\n    if len(remaining_parts) != 2:  # Training set (includes roles)\n        parts.append(remaining_parts[2])\n        parts.append(\" \".join(str(item) for item in remaining_parts[3:]))\n    \n    return parts\n\n\ndef load_annotations(file_path):\n    with open(file_path, \"r\") as f:\n        lines = f.readlines()\n    processed_lines = [\n        extract_entity_and_fine_grained_roles(line.strip().split(\"\\t\"))\n        for line in lines\n    ]\n    return pd.DataFrame(\n        processed_lines,\n        columns=[\n            \"article_id\",\n            \"entity\",\n            \"start_offset\",\n            \"end_offset\",\n            \"main_role\",\n            \"fine_grained_roles\",\n        ],\n    )\n\n\ndef load_document(article_id, folder_path):\n    file_path = os.path.join(folder_path, article_id)\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\n\n# Data Preprocessing: Adding special token around entities\ndef preprocess_entity(entity, context, start_offset, end_offset, tokenizer):\n    context_with_entity = context[:start_offset] + f\"<ENTITY>{entity}</ENTITY>\" + context[end_offset:]\n    inputs = tokenizer(\n        context_with_entity,\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    return inputs\n\n\n# Dataset Class with Hierarchical Classification\nclass RoleDataset(Dataset):\n    def __init__(self, annotations, documents, tokenizer, max_length=512):\n        self.annotations = annotations\n        self.documents = documents\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        annotation = self.annotations.iloc[idx]\n        article_id = annotation[\"article_id\"]\n        entity = annotation[\"entity\"]\n        start_offset = int(annotation[\"start_offset\"])\n        end_offset = int(annotation[\"end_offset\"])\n        main_role = annotation[\"main_role\"]\n        fine_grained_roles = annotation[\"fine_grained_roles\"].split(\" \")\n\n        # Get the document text\n        document = self.documents[article_id]\n\n        # Preprocess document to add special entity tokens\n        inputs = preprocess_entity(entity, document, start_offset, end_offset, self.tokenizer)\n\n        # Label encoding\n        all_main_roles = set(main_roles)\n        all_fine_grained_roles = set(individual_fine_grained_roles)\n\n        main_role_label = [main_role]\n        fine_grained_roles_labels = fine_grained_roles\n\n        mlb_main = MultiLabelBinarizer(classes=list(all_main_roles))\n        mlb_fine = MultiLabelBinarizer(classes=list(all_fine_grained_roles))\n\n        main_role_encoded = mlb_main.fit_transform([main_role_label])[0]\n        fine_grained_encoded = mlb_fine.fit_transform([fine_grained_roles_labels])[0]\n\n        # Combine labels\n        labels = torch.cat(\n            [\n                torch.tensor(main_role_encoded, dtype=torch.float),\n                torch.tensor(fine_grained_encoded, dtype=torch.float),\n            ]\n        )\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n            \"labels\": labels,\n        }\n\n\n# Main Model with Hierarchical Classification\n# class RoleClassifier(nn.Module):\n#     def __init__(self, model_name, num_main_roles, num_fine_grained_roles):\n#         super(RoleClassifier, self).__init__()\n#         self.base_model = AutoModel.from_pretrained(model_name)\n#         hidden_size = self.base_model.config.hidden_size\n#         self.main_role_classifier = nn.Linear(hidden_size, num_main_roles)\n#         self.fine_grained_classifier = nn.Linear(hidden_size, num_fine_grained_roles)\n\n#     def forward(self, input_ids, attention_mask, labels=None):\n#         outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n#         cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token representation\n\n#         main_role_logits = self.main_role_classifier(cls_output)\n#         fine_grained_logits = self.fine_grained_classifier(cls_output)\n\n#         if labels is not None:\n#             main_role_labels = labels[:, :3]  # First 3 for main roles\n#             fine_grained_labels = labels[:, 3:]  # Remaining for fine-grained roles\n\n#             main_loss = BCEWithLogitsLoss()(main_role_logits, main_role_labels)\n#             fine_loss = BCEWithLogitsLoss()(fine_grained_logits, fine_grained_labels)\n\n#             loss = main_loss + fine_loss\n#             return loss, main_role_logits, fine_grained_logits\n\n#         return main_role_logits, fine_grained_logits\n\nclass RoleClassifier(nn.Module):\n    def __init__(self, model_name, main_roles, fine_grained_roles):\n        super(RoleClassifier, self).__init__()\n        self.base_model = AutoModel.from_pretrained(model_name)\n        hidden_size = self.base_model.config.hidden_size\n        self.main_role_classifier = nn.Linear(hidden_size, len(main_roles))\n        self.fine_grained_classifiers = nn.ModuleDict({\n            main_role: nn.Linear(hidden_size, len(fine_roles)) \n            for main_role, fine_roles in fine_grained_roles.items()\n        })\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        # Get the output from the transformer model\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token representation\n        \n        # Main role prediction\n        main_role_logits = self.main_role_classifier(cls_output)\n        main_role_probs = softmax(main_role_logits, dim=-1)\n\n        # Determine fine-grained roles based on the predicted main role\n        fine_grained_logits = None\n        if labels is not None:\n            main_role_labels = labels[:, :len(main_roles)]\n            fine_grained_labels = labels[:, len(main_roles):]\n\n            # Calculate loss\n            main_loss = BCEWithLogitsLoss()(main_role_logits, main_role_labels)\n            fine_loss = 0\n            return main_loss + fine_loss, main_role_logits, fine_grained_logits\n        \n        # Get the fine-grained roles classifier based on the predicted main role\n        main_role_idx = np.argmax(main_role_probs.cpu().numpy(), axis=-1)\n        # main_role = list(main_roles.keys())[main_role_idx]  # Get the corresponding main role\n        main_role = main_roles[main_role_idx]  # Access the main role directly from the list\n\n        # Only apply fine-grained classification for the predicted main role\n        fine_grained_logits = self.fine_grained_classifiers[main_role](cls_output)\n\n        return main_role_logits, fine_grained_logits\n\n\n# Initialize the tokenizer and dataset\nmodel_name = \"xlm-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load annotations and documents\nannotations = load_annotations(train_annotations_file)\ndocuments = {\n    article_id: load_document(article_id, train_raw_documents_folder)\n    for article_id in annotations[\"article_id\"].unique()\n}\n\ntrain_dataset = RoleDataset(annotations, documents, tokenizer)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n\n\n\n# Fine-grained roles and main roles\nmain_roles = [\"Protagonist\", \"Antagonist\", \"Innocent\"]\n\nindividual_fine_grained_roles = ['Guardian', 'Martyr', 'Peacemaker', 'Rebel', 'Underdog', 'Virtuous',\n            'Instigator', 'Conspirator', 'Tyrant', 'Foreign Adversary', 'Traitor', \n            'Spy', 'Saboteur', 'Corrupt', 'Incompetent', 'Terrorist', 'Deceiver', 'Bigot',\n            'Forgotten', 'Exploited', 'Victim', 'Scapegoat']\n\nfine_grained_roles = {\n    \"Protagonist\": [\"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Underdog\", \"Virtuous\"],\n    \"Antagonist\": [\"Instigator\", \"Conspirator\", \"Tyrant\", \"Foreign Adversary\", \"Traitor\", \n                   \"Spy\", \"Saboteur\", \"Corrupt\", \"Incompetent\", \"Terrorist\", \"Deceiver\", \"Bigot\"],\n    \"Innocent\": [\"Forgotten\", \"Exploited\", \"Victim\", \"Scapegoat\"]\n}\n\n\n# Initialize the model\nnum_main_roles = len(main_roles)\nnum_fine_grained_roles = len(individual_fine_grained_roles)\n\n\n# model = RoleClassifier(model_name, num_main_roles, num_fine_grained_roles)\nmodel = RoleClassifier(model_name, main_roles, fine_grained_roles)\n\n\n# Training setup\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"no\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    save_total_limit=1,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n)\n\n\n# Train the model\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model\ntorch.save(model.state_dict(), \"./role_classifier.pth\")\ntokenizer.save_pretrained(\"./role_classifier\")\n\ntorch.save(model.state_dict(), \"./role_classifier/pytorch_model.bin\")\n\n# Save the model configuration\nmodel_config = {\n    \"model_name\": model_name,\n    \"num_main_roles\": num_main_roles,\n    \"num_fine_grained_roles\": num_fine_grained_roles,\n}\ntorch.save(model_config, \"./role_classifier/model_config.pth\")\n\n\n# Example usage with model loading and prediction\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = RoleClassifier(model_name, num_main_roles, num_fine_grained_roles)\nmodel = RoleClassifier(model_name, main_roles, fine_grained_roles)\n\nmodel.load_state_dict(torch.load(\"./role_classifier/pytorch_model.bin\"))\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dev_raw_documents_folder = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-documents\"\n\nsigmoid_threshold = 0.1\n\n# Load dev annotations\ndef load_dev_annotations(file_path):\n    with open(file_path, \"r\") as f:\n        lines = f.readlines()\n    processed_lines = [\n        extract_entity_and_fine_grained_roles(line.strip().split(\"\\t\"))\n        for line in lines\n    ]\n    return pd.DataFrame(\n        processed_lines, columns=[\"article_id\", \"entity\", \"start_offset\", \"end_offset\"]\n    )\n\n\n# Dev Dataset\nclass DevRoleDataset(Dataset):\n    def __init__(self, annotations, documents, tokenizer, max_length=512):\n        self.annotations = annotations\n        self.documents = documents\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        annotation = self.annotations.iloc[idx]\n        article_id = annotation[\"article_id\"]\n        entity = annotation[\"entity\"]\n        start_offset = int(annotation[\"start_offset\"])\n        end_offset = int(annotation[\"end_offset\"])\n\n        \n        # Get the document text\n        document = self.documents[article_id]\n\n\n        # ENTITY CENTER\n# Center the entity in the context window\n        entity_start_char_idx = start_offset\n        entity_end_char_idx = end_offset\n\n        # Define a character window around the entity\n        half_window_size = self.max_length // 2\n        left_context_start = max(0, entity_start_char_idx - half_window_size)\n        right_context_end = min(len(document), entity_end_char_idx + half_window_size)\n\n        # Slice the document around the entity\n        context = document[left_context_start:right_context_end]\n\n        # Adjust the entity offsets for the new context\n        adjusted_entity_start = entity_start_char_idx - left_context_start\n        adjusted_entity_end = entity_end_char_idx - left_context_start\n\n\n\n\n        context_with_entity = context[:start_offset] + f\"<ENTITY>{entity}</ENTITY>\" + context[end_offset:]\n\n        \n        \n        # Tokenize and prepare inputs\n        inputs = self.tokenizer(\n            # document,\n            # context,\n            context_with_entity,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n\n            \"article_id\": article_id,\n            \"entity\": entity,\n            \"start_offset\": start_offset,\n            \"end_offset\": end_offset,\n        }\n\n\n# def predict_roles_hierarchical(model, dataloader, tokenizer):\n#     model.eval()\n#     predictions = []\n\n#     with torch.no_grad():\n#         for batch in dataloader:\n#             input_ids = batch[\"input_ids\"].to(\"cuda\")\n#             attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n\n#             # Main role prediction (first model for coarse classification)\n#             main_role_logits, fine_grained_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n\n#             # Apply softmax for main role classification\n#             main_role_probs = softmax(main_role_logits, dim=-1).cpu().numpy()\n#             main_role_preds = np.argmax(main_role_probs, axis=-1)\n\n#             # Fine-grained role prediction (based on the predicted main role)\n#             for i in range(len(input_ids)):\n#                 main_role = main_roles[main_role_preds[i]]\n#                 fine_grained_logits_for_role = fine_grained_logits[i]\n#                 fine_grained_preds = (torch.sigmoid(fine_grained_logits_for_role) > sigmoid_threshold).cpu().numpy()\n\n            \n            \n#                 # # Map fine-grained roles based on the predicted main role\n                \n#                 # # Example: A mapping of main roles to their index ranges in fine_grained_preds\n#                 # # Replace these ranges with the actual ranges for your use case\n#                 # main_role_index_bounds = {\n#                 #     \"Protagonist\": (0, 6),  # Indices for Protagonist fine-grained roles (exclusive at end)\n#                 #     \"Antagonist\": (6, 18),  # Indices for Antagonist fine-grained roles\n#                 #     \"Innocent\": (18, 22),   # Indices for Innocent fine-grained roles\n#                 #     # Add other roles and their bounds here\n#                 # }\n                \n#                 # # Get the start and end index for the main_role\n#                 # start_idx, end_idx = main_role_index_bounds[main_role]\n                \n#                 # # Filter fine-grained roles for the main_role based on its index range\n#                 # fine_grained_roles_predicted = [\n#                 #     fine_grained_roles[main_role][idx - start_idx]  # Adjust index relative to main_role's fine-grained roles\n#                 #     for idx, val in enumerate(fine_grained_preds[start_idx:end_idx], start=start_idx)  # Iterate over specific range\n#                 #     if val == 1  # Pick only the roles predicted as True\n#                 # ]\n\n                \n\n            \n#                 # # Collect results\n#                 # predictions.append(\n#                 #     {\n#                 #         \"article_id\": batch[\"article_id\"][i],\n#                 #         \"entity\": batch[\"entity\"][i],\n#                 #         \"start_offset\": batch[\"start_offset\"][i],\n#                 #         \"end_offset\": batch[\"end_offset\"][i],\n#                 #         \"main_roles\": [main_roles[main_role_preds[i]]],\n#                 #         \"fine_grained_roles\": fine_grained_roles_predicted,\n#                 #     }\n#                 # )\n    \n    \n#                 # Map fine-grained roles based on the predicted main role\n#                 # Get the valid fine-grained roles for the predicted main role\n#                 valid_fine_grained_roles = fine_grained_roles_for_main[main_role]\n                \n#                 # Map the fine-grained predictions to the corresponding fine-grained roles\n#                 valid_fine_grained_indices = [\n#                     individual_fine_grained_roles.index(role) for role in valid_fine_grained_roles\n#                 ]\n\n#                 # Set the fine-grained predictions for invalid roles to 0 (inactive)\n#                 final_fine_grained_preds = np.zeros_like(fine_grained_preds)\n#                 for idx in valid_fine_grained_indices:\n#                     final_fine_grained_preds[idx] = fine_grained_preds[idx]\n\n#                 # Store the results\n#                 predictions.append({\n#                     \"article_id\": batch[\"article_id\"][i],\n#                     \"entity\": batch[\"entity\"][i],\n#                     \"main_role\": main_role,\n#                     \"fine_grained_roles\": [\n#                         fine_grained_roles[idx] for idx in range(len(final_fine_grained_preds)) if final_fine_grained_preds[idx] > 0.5\n#                     ],\n#                 })\n\n\n\n#     return predictions\n\n\ndef predict_roles_hierarchical(model, dataloader, tokenizer):\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(\"cuda\")\n            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n\n            # Main role prediction (first model for coarse classification)\n            main_role_logits, fine_grained_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            # Apply softmax for main role classification\n            main_role_probs = softmax(main_role_logits, dim=-1).cpu().numpy()\n            main_role_preds = np.argmax(main_role_probs, axis=-1)\n\n            # Get the fine-grained roles based on predicted main role\n            fine_grained_preds = []\n            for idx in range(len(main_role_preds)):\n                main_role = main_roles[main_role_preds[idx]]  # Get predicted main role\n                fine_grained_probs = softmax(fine_grained_logits[idx], dim=-1).cpu().numpy()\n                fine_grained_preds.append(fine_grained_probs)\n\n            # Create prediction dictionary\n            predictions.append({\n                'article_id': batch[\"article_id\"],\n                'entity': batch[\"entity\"],\n                'main_role': main_role_preds,\n                'fine_grained_roles': fine_grained_preds\n            })\n\n    return predictions\n\n\n\n# Update the prediction function to save predictions in the desired format\ndef save_predictions(predictions, output_file):\n    with open(output_file, \"w\") as f:\n        for prediction in predictions:\n            article_id = prediction[\"article_id\"]\n            entity = prediction[\"entity\"]\n            start_offset = prediction[\"start_offset\"]  # Start offset for the entity\n            end_offset = prediction[\"end_offset\"]  # End offset for the entity\n            main_role = prediction[\"main_roles\"][0]  # Assume we are picking only one main_role\n            fine_grained_roles = \"\\t\".join(prediction[\"fine_grained_roles\"])  # Join fine-grained roles with tab\n\n            # Prepare the line in the required format\n            line = f\"{article_id}\\t{entity}\\t{start_offset}\\t{end_offset}\\t{main_role}\\t{fine_grained_roles}\\n\"\n            f.write(line)\n\n    print(f\"Predictions saved to {output_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(main_roles)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dev_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-entity-mentions_EN.txt\"\n\n\n# Load dev annotations and documents\ndev_annotations = load_dev_annotations(dev_annotations_file)\ndev_documents = {\n    article_id: load_document(article_id, dev_raw_documents_folder)\n    for article_id in dev_annotations[\"article_id\"].unique()\n}\n\n# Dev Dataset and DataLoader\ndev_dataset = DevRoleDataset(dev_annotations, dev_documents, tokenizer)\ndev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n\n\n\n# Assuming `dev_loader` is the DataLoader for the dev set\npredictions = predict_roles_hierarchical(model, dev_loader, tokenizer)\n\n# Save predictions to .txt file\nsave_predictions(predictions,\"submission_EN.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dev_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-entity-mentions_BG.txt\"\n\n\n# Load dev annotations and documents\ndev_annotations = load_dev_annotations(dev_annotations_file)\ndev_documents = {\n    article_id: load_document(article_id, dev_raw_documents_folder)\n    for article_id in dev_annotations[\"article_id\"].unique()\n}\n\n# Dev Dataset and DataLoader\ndev_dataset = DevRoleDataset(dev_annotations, dev_documents, tokenizer)\ndev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n\n\n\n\n# Assuming `dev_loader` is the DataLoader for the dev set\npredictions = predict_roles_hierarchical(model, dev_loader, tokenizer)\n\n# Save predictions to .txt file\nsave_predictions(predictions,\"submission_BG.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dev_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-entity-mentions_HI.txt\"\n\n\n# Load dev annotations and documents\ndev_annotations = load_dev_annotations(dev_annotations_file)\ndev_documents = {\n    article_id: load_document(article_id, dev_raw_documents_folder)\n    for article_id in dev_annotations[\"article_id\"].unique()\n}\n\n# Dev Dataset and DataLoader\ndev_dataset = DevRoleDataset(dev_annotations, dev_documents, tokenizer)\ndev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n\n\n\n# Assuming `dev_loader` is the DataLoader for the dev set\npredictions = predict_roles_hierarchical(model, dev_loader, tokenizer)\n\n# Save predictions to .txt file\nsave_predictions(predictions,\"submission_HI.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dev_annotations_file = \"/kaggle/input/processed-5/assignment2_processed_dataset/dev-documents_25_October/subtask-1-entity-mentions_PT.txt\"\n\n\n# Load dev annotations and documents\ndev_annotations = load_dev_annotations(dev_annotations_file)\ndev_documents = {\n    article_id: load_document(article_id, dev_raw_documents_folder)\n    for article_id in dev_annotations[\"article_id\"].unique()\n}\n\n# Dev Dataset and DataLoader\ndev_dataset = DevRoleDataset(dev_annotations, dev_documents, tokenizer)\ndev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n\n\n# Assuming `dev_loader` is the DataLoader for the dev set\npredictions = predict_roles_hierarchical(model, dev_loader, tokenizer)\n\n# Save predictions to .txt file\nsave_predictions(predictions,\"submission_PT.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}